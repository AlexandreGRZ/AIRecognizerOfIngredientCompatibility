{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e8fc45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "7fc132ba",
   "metadata": {},
   "source": [
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "\n",
    "INPUT_FILE  = './data/openfoodfacts-products.jsonl.gz'\n",
    "OUTPUT_FILE = './data/ingredients.csv'\n",
    "\n",
    "with gzip.open(INPUT_FILE, 'rt', encoding='utf-8') as source, \\\n",
    "     open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as target:\n",
    "\n",
    "    writer = csv.writer(target)\n",
    "    writer.writerow(['code', 'ingredients'])\n",
    "\n",
    "    for line in source:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            product = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # ligne corrompue ou incomplète : on l'ignore\n",
    "            continue\n",
    "\n",
    "        # on préfère le texte français si dispo\n",
    "        ing = product.get('ingredients_text_fr') or product.get('ingredients_text')\n",
    "        if ing:\n",
    "            writer.writerow([\n",
    "                product.get('code', ''),\n",
    "                ing.replace('\\n', ' ').strip()\n",
    "            ])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1848d682",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "id": "034894d5",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Lire le CSV\n",
    "df = pd.read_csv(\n",
    "    './data/ingredients.csv',\n",
    "    dtype={'ingredients': str},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "\n",
    "df['ingredients'] = df['ingredients'].fillna('')\n",
    "\n",
    "# 2. Nettoyage et tokenisation simple\n",
    "def clean_and_tokenize(text):\n",
    "    # minuscules, retirer ponctuation sauf ‘;’\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9éèàçùœ \\-;]', ' ', text)\n",
    "    # split sur ‘;’ puis strip des blancs\n",
    "    return [tok.strip() for tok in text.split(';') if tok.strip()]\n",
    "\n",
    "df['tokens'] = df['ingredients'].apply(clean_and_tokenize)\n",
    "\n",
    "print(df[['ingredients','tokens']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "60e25a655d327a86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = df['tokens'].tolist()\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "vec_tomate = w2v.wv['tomate']\n"
   ],
   "id": "b44684a8caa9a322",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "74bdc9545a3aa979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def list_embedding(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "df['list_emb'] = df['tokens'].apply(lambda toks: list_embedding(toks, w2v))"
   ],
   "id": "81f4da3ecfeba7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3d981e66dc4e374e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = np.vstack(df['list_emb'].values)\n",
    "\n",
    "y = df['score'].values / 100.0\n"
   ],
   "id": "2626fd4233e774a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "854506a1b9d45b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "class ScoringNet(nn.Module):\n",
    "    def __init__(self, emb_dim=100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),   # sortie scalaire\n",
    "            nn.Sigmoid()        # donne un score dans [0,1]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Préparer DataLoader…\n",
    "# Convertir X_train, y_train en TensorDataset puis DataLoader\n",
    "# Boucle d’entraînement classique avec MSELoss et AdamW\n",
    "\n"
   ],
   "id": "875c69ff38817bc2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
