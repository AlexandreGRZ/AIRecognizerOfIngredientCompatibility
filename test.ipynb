{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e8fc45",
   "metadata": {},
   "source": [
    "Ce notebook montre comment préparer des données Open Food Facts pour entraîner un modèle évaluant la compatibilité d'une liste d'ingrédients.\n",
    "\n",
    "Le bloc suivant extrait du fichier JSONL compressé un CSV `ingredients.csv` plus facile à manipuler.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fc132ba",
   "metadata": {},
   "source": [
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "\n",
    "INPUT_FILE  = './data/openfoodfacts-products.jsonl.gz'\n",
    "OUTPUT_FILE = './data/ingredients.csv'\n",
    "\n",
    "with gzip.open(INPUT_FILE, 'rt', encoding='utf-8') as source, \\\n",
    "     open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as target:\n",
    "\n",
    "    writer = csv.writer(target)\n",
    "    writer.writerow(['code', 'ingredients'])\n",
    "\n",
    "    for line in source:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            product = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # ligne corrompue ou incomplète : on l'ignore\n",
    "            continue\n",
    "\n",
    "        # on préfère le texte français si dispo\n",
    "        ing = product.get('ingredients_text_fr') or product.get('ingredients_text')\n",
    "        if ing:\n",
    "            writer.writerow([\n",
    "                product.get('code', ''),\n",
    "                ing.replace('\\n', ' ').strip()\n",
    "            ])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1848d682",
   "metadata": {},
   "source": [
    "**Chargement et nettoyage** : on ouvre `ingredients.csv`, on met le texte en minuscules, on retire la ponctuation superflue puis on découpe les listes en tokens (colonne `tokens`).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "034894d5",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Lire le CSV\n",
    "df = pd.read_csv(\n",
    "    './data/ingredients.csv',\n",
    "    dtype={'ingredients': str},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "\n",
    "df['ingredients'] = df['ingredients'].fillna('')\n",
    "\n",
    "# 2. Nettoyage et tokenisation simple\n",
    "def clean_and_tokenize(text):\n",
    "    # minuscules, retirer ponctuation sauf ‘;’\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9éèàçùœ \\-;]', ' ', text)\n",
    "    # split sur ‘;’ puis strip des blancs\n",
    "    return [tok.strip() for tok in text.split(';') if tok.strip()]\n",
    "\n",
    "df['tokens'] = df['ingredients'].apply(clean_and_tokenize)\n",
    "\n",
    "print(df[['ingredients','tokens']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**EntraÃ®nement d'un modÃ¨le Word2Vec** pour obtenir des vecteurs reprÃ©sentant chaque ingrÃ©dient Ã  partir des tokens.\n",
    "\n",
    "**Optimisation** : on rÃ¨gle `workers=os.cpu_count()` pour exploiter tous les cÅurs du processeur. Gensim ne profite pas directement du GPU (RTXÂ 2050) mais la parallÃ©lisation CPU rÃ©duit nettement le temps d'Ã©ntraÃ®nement sur un i9.\n"
   ],
   "id": "60e25a655d327a86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = df['tokens'].tolist()\n",
    "\n",
    "num_workers = os.cpu_count()  # utilise tous les coeurs\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    epochs=10,\n",
    "    workers=num_workers,\n",
    ")\n",
    "\n",
    "vec_tomate = w2v.wv['tomate']\n"
   ],
   "id": "b44684a8caa9a322",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Embedding moyen par produit** : on calcule la moyenne des vecteurs d'ingrédients pour chaque liste (`list_emb`).\n"
   ],
   "id": "74bdc9545a3aa979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def list_embedding(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "df['list_emb'] = df['tokens'].apply(lambda toks: list_embedding(toks, w2v))"
   ],
   "id": "81f4da3ecfeba7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score de compatibilité automatique** : on mesure la similarité moyenne entre toutes les paires d'ingrédients pour produire un score dans l'intervalle [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compatibility_score(tokens, model):\n",
    "    pairs = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i+1, len(tokens)):\n",
    "            if tokens[i] in model.wv and tokens[j] in model.wv:\n",
    "                pairs.append(model.wv.similarity(tokens[i], tokens[j]))\n",
    "    if not pairs:\n",
    "        return 0.5\n",
    "    sim = float(np.mean(pairs))\n",
    "    return (sim + 1) / 2\n",
    "\n",
    "df['score'] = df['tokens'].apply(lambda toks: compatibility_score(toks, w2v))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Préparation des données d'entraînement** : `X` regroupe les embeddings moyens et `y` contient les scores calculés précédemment.\n"
   ],
   "id": "3d981e66dc4e374e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = np.vstack(df['list_emb'].values)\n",
    "\n",
    "y = df['score'].values\n"
   ],
   "id": "2626fd4233e774a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Entraînement du réseau** : on définit `ScoringNet`, construit un DataLoader puis on entraîne le modèle quelques itérations. Si un GPU est disponible, le modèle et les batchs y sont transférés pour accélérer l'entraînement.\n"
   ],
   "id": "854506a1b9d45b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class ScoringNet(nn.Module):\n",
    "    def __init__(self, emb_dim=100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
    "ds = TensorDataset(X_t, y_t)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=os.cpu_count(), pin_memory=(device.type=='cuda'))\n",
    "\n",
    "model = ScoringNet(X.shape[1]).to(device)\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = crit(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(f'epoch {epoch} loss {loss.item():.4f}')\n"
   ],
   "id": "875c69ff38817bc2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
